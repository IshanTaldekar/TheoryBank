---
tags:
  - b-plus-trees
  - interesting
  - low-level
  - memory
---
## $B^+-Tree$ File Organization
==As mentioned in Section 11.3, the main drawback of index-sequential file organization is the degradation of performance as file grows: With growth, an increasing percentage of index entries and actual records become out of order, and are stored in overflow blocks. We solve the degradation of index lookups using $B^+-tree$ indices on the file.== We solve the ==degradation problem for storing the actual records by using the leaf level of the $B^+-tree$ to organize the blocks containing the actual records. We use the $B^+-tree$ structure not only as an index, but also as an organizer for records in a file. In a **$B^+-tree$ file organization**, the lead nodes of the tree store records, instead of storing pointers to records==. Figure 11.20 shows an example of a $B^+-tree$ file organization. ==Since records are usually larger than pointers, the maximum number of records that can be stored in a leaf node is less than the number of pointers in a non-leaf node. However, the leaf nodes are still required to be at least half full.==

![[Pasted image 20240506113540.png]]

Insertion and deletion of records from a $B^+-tree$ file organization are handled in the same way as insertion and deletion of entries in a $B^+-tree$ index. When a record with a given key value *v* is inserted, the system locates the block that should contain the record by searching the $B^+-tree$ for the largest key in the tree that is $\le v$. If the block located has enough free space for the record, the system stores the record in the block. Otherwise, as in $B^+-tree$ insertion, the system splits the block into two, and redistributes the records in it (in the $B^+-tree-key\ order$) to create space for the new record. The split propagates up the $B^+-tree$ in the normal fashion. When we delete a record, the system first removes it from the block containing it. If a block *B* becomes less than half full as a result, the records in *B* are redistributed with the records in an adjacent block *B'*. Assuming fixed-sized records, each block will hold at least one-half as many records as the maximum that it can hold. The system updates the non-leaf nodes of the $B^+-tree$ in the usual fashion.

==When we use a $B^+-tree$ for file organization, space utilization is particularly important, since the space occupied by the records is likely to be much more than the space occupied by the keys and pointers. We can improve the utilization of space in a $B^+-tree$ by involving more sibling nodes in redistribution during splits and merges. The technique is applicable to both leaf nodes and non-leaf nodes, and works as follows:==

During insertion, if a node is full the system attempts to redistribute some of its entries to one of the adjacent nodes, to make space  for a new entry. If this attempt fails because the adjacent nodes are themselves full, the system splits the node, and split the entries evenly among one of the adjacent nodes and the two nodes that it obtained by splitting the original node. Since the three nodes together contain one more record than can fit in two nodes, each node will be about two-thirds full. More precisely, each node will have at least $\lfloor 2n/3 \rfloor$ entries, where *n* is the maximum number of entries that the node can hold. ($\lfloor x \rfloor$ denotes the greatest integer that is less than or equal to *x*; that is, we drop the fractional part, if any.) 

During deletion of a record, if the occupancy of a node falls below $\lfloor 2n/3 \rfloor$, the system attempts to borrow an entry from one of the sibling nodes. If both the sibling nodes have $\lfloor 2n/3 \rfloor$ records, instead of borrowing an entry, the system redistributes the entries in the nod and in the two siblings evenly between two of the nodes, and deletes the third node. We can use this approach because the total number of entries is $3 \lfloor 2n/3 \rfloor - 1$, which is less than *2n*. With three adjacent nodes used for redistribution, each node can be guaranteed to have $\lfloor 3n/4 \rfloor$ entries. In general, if *m* nodes (*m* - 1 siblings) are involved in redistribution, each node can be guaranteed to contain at least $\lfloor (m - 1) n/m \rfloor$ entries. However, the cost of update becomes higher as more sibling nodes are involved in the redistribution.

Note that in a $B^+-tree$ index or file organization, leaf nodes that are adjacent to each other in the tree may be located at different places on disk. When a file organization is newly created on a set of records, it is possible to allocate blocks that are mostly contiguous on disk to leaf nodes that are contiguous in the tree. Thus a sequential scan of leaf nodes would correspond to a mostly sequential scan on disk. As insertions and deletions occur on the tree, sequentiality is increasingly lost, and sequential access has to wait for disk seeks increasingly often. And index rebuild may be required to restore sequentiality.

$B^+-tree$ file organizations can also be used to store large objects, such as SQL clobs and blobs, which may be larger than a disk block, and as large as multiple gigabytes. Such large objects can be stored by splitting them into sequences of smaller records that are organized in a $B^+-tree$ file organization. The records can be sequentially numbered, or numbered by the byte offset of the record within the large object, and the record number can be used as the search key.
## Secondary Indices and Record Relocation
Some file organizations, such as the $B^+-tree$ file organization, may change the location of records even when the records have not been updated. As an example, when a leaf node is split in a $B^+-tree$ file organization, a number of records are moved to a new node. In such cases, all secondary indices that store pointers to the relocated records would have to be updated, even though the values in the records may not have changed. Each leaf node may contain a fairly large number of records, and each of them may be in different locations on each secondary index. Thus a leaf-node split may require tens or even hundreds of I/O operations to update all affected secondary indices, making it an expensive operation.

A widely used solution for this problem is as follows: In secondary indices, in place of pointers to the indexed records, we store the values of the primary-index search-key attributes. For example, suppose we have a primary index on the attribute *ID* of relation *instructor*; then a secondary index on *dept_name* would store with each department name a list of instructor's *ID* values of the corresponding records, instead of storing pointers to the records.

Relocation of records because of leaf-node splits then does not require any update on any such secondary index. However, locating a record using the secondary index now requires two steps: First we use the secondary index to find the primary-index search-key values, and then use the primary index to find the corresponding records.

The above approach thus greatly reduces the cost of index update due to file reorganization, although it increases the cost of accessing data using a secondary index.
## Indexing Strings
Creating $B^+-tree$ indices on string-valued attributes raises two problems. The first problem is that string can be of variable length. The second problem is that strings can be long, leading to a low fanout and a correspondingly increased tree height. 

With variable-length search keys, different nodes can have different fanouts even if they are full. A node must then be split if it is full, that is, there is no space to add a new entry, regardless of how many search entries it has. Similarly, nodes can be merged or entries redistributed depending on what fraction of the space in the nodes is used, instead of being based on the maximum number of entries that the node can hold.

The fanout of nodes can be increased by using a technique called **prefix compression**. With prefix compression, we do not store the entire search key value at non-leaf nodes. We only store a prefix of each search key value that is sufficient to distinguish between the key values in the subtree that it separates. For example, if we had an index on names, the key value at a non-leaf node could be a prefix of a name; it may suffice to store "Silb" at a non-leaf node, instead of the full "Silberschatz" if the closest value in the two sub-trees that it separates are, say, "Silas" and "Silver" respectively.
## Bulk Loading of $B^+-Tree$ Indices
As we saw earlier, insertion of a record in a $B^+-tree$ requires a number of I/O operations that in the worst case is proportional to the height of the tree, which is usually fairly small (typically five or less, even for large relations).

Now consider the case where a $B^+-tree$ is being built on a large relation. Suppose the relation is significantly larger than main memory, and we are constructing a non-clustering index on the relation such that the index is also larger than main memory. In this case, as we scan the relation and add entries to the $B^+-tree$, it is quite likely that each leaf node accessed is not in the database buffer when it is accessed, since there is no particular ordering of the entries. With such randomly ordered accesses to blocks, each time an entry is added to the leaf, a disk seek will be required to fetch the block containing the leaf node. The block will probably be evicted from the disk buffer before another entry is added to the block, leading to another disk seek to write the block back to disk. Thus a random read and a random write operation may be required for each entry inserted.

For example, if the relation has 100 million records, and each I/O operation takes about 10 milliseconds, it would take at least 1 million seconds to build the index, counting only the cost of reading leaf nodes, not even counting the cost of writing the updating nodes back to disk. This is clearly a very large amount of time; in contrast, if each record occupies 100 bytes, and the disk subsystem can transfer data at 50 megabytes per second, it would take just 200 seconds to read the entire relation.

Insertion of a large number of entries at a time into an index is referred to as **bulk loading** of the index. An efficient way to perform bulk loading of an index is as follows. First, create a temporary file containing index entries for the relation, then sort the file on the search key of the index being constructed, and finally scan the sorted file and insert the entries into the index. There are efficient algorithms for sorting large relations, which are described in [[Chapter 12]], which can sort even a large file with an I/O cost comparable to that of reading the file a few times, assuming a reasonable amount of main memory is available.

There are significant benefits to sorting the entries before inserting them into the $B^+-tree$. When the entries are inserted in sorted order, all entries that go to a particular leaf node will appear consecutively, and the leaf needs to be written only once; nodes will never have to be read from disk during bulk load, if the $B^+-tree$ was empty to start with. Each leaf node will thus incur only one I/O operation even though many entries may be inserted into the node. If each leaf contains 100 entries, the leaf level will contain 1 million nodes, resulting in only 1 million I/O operations for creating the leaf level. Even these I/O operations can be expected to be sequential, if successive leaf nodes are allocated on successive disk blocks, and few disk seeks would be required. With current disks, 1 millisecond per block is a reasonable estimate for most sequential I/O operations, in contrast to 10 milliseconds per block for random I/O operations.

We study the cost of sorting a large relation in [[Chapter 12]], but as a rough estimate, the index which would have taken a million seconds to build otherwise, can be constructed in well under 1000 seconds by sorting the entries before inserting them into the $B^+-tree$, in contrast to more than 1,000,000 seconds for inserting in random order.

If the $B^+-tree$ is initially empty, it can be constructed faster by building it bottom-up, from the leaf level, instead of using the usual insert procedure. In **bottom-up $B^+-tree$ construction**, after sorting the entries as we just described, we break up the sorted entries into blocks, keeping as many entries in a block as can fit in the block; the resulting blocks form the leaf level of the $B^+-tree$. The minimum value in each block, along with the pointer to the block, is used to create entries in the next level of the $B^+-tree$, pointing to the leaf blocks. Each further level of the tree is similarly constructed using the minimum values associated with each node one level below, until the root is created. 

Most database systems implement efficient techniques based on sorting of entries, and bottom-up construction, when creating an index on a relation, although they use the normal insertion procedure when tuples are added one at a time to a relation with an existing index. Some database systems recommend that if a very large number of tuples are added at once to an already existing relation, indices on the relation (other than any index on the primary key) should be dropped, and then re-created after the tuples are inserted, to take advantage of efficient bulk-loading techniques.
## B-Tree Index Files
**B-tree indices** are similar to $B^+-tree$ indices. The primary distinction between the two approaches is that a B-tree eliminates the redundant storage of search-key values. In the $B^+-tree$ of Figure 11.3, the search key "Califieri", "Einstein", "Gold", "Mozart", and "Srinivasan" appear in non-leaf nodes, in addition to appearing in the leaf nodes. Every search-key value appears in some leaf node; several are repeated in non-leaf nodes.

![[Pasted image 20240506142207.png]]

A B-tree allows search values to appear only once (if they are unique), unlike a $B^+-tree$, where a value may appear in a non-leaf node, in addition to appearing in a leaf node. Figure 11.21 shows a B-tree that represents the same search keys as the $B^+-tree$ of Figure 11.13. Since search keys are not repeated in the B-tree, we may be able to store the index in fewer tree nodes than in the corresponding $B^+-tree$ index. However, since search keys that appear in non-leaf nodes appear nowhere else in the B-tree, we are forced to include an additional pointer field for each search key in a non-leaf node. These additional pointers point to either file records or buckets for the associated search key.

It is worth noting that many database system manuals, articles in industry literature, and industry professionals use the term B-tree to refer to the data structure that we call the $B^+-tree$. In fact, it would be fair to say that in current usage, the term B-tree is assumed to be synonymous with $B^+-tree$. However, in this book we use the terms B-tree and $B^+-tree$ as they were originally defined, to avoid confusion between the two data structures.

![[Pasted image 20240506143954.png]]

A generalized B-tree leaf node appears in Figure 11.22a; a non-leaf node appears in Figure 11.22b. Leaf nodes are the same as in $B^+-trees$. In non-leaf nodes, the pointers $P_i$ are the tree pointers that we used also for $B^+-trees$, while the pointers $B_i$ are bucket or file pointers. In the generalized B-tree in the figure, there are *n - 1* keys in the leaf node, but there are *m - 1* keys in the non-leaf node. This discrepancy occurs because non-leaf nodes must include pointers $B_i$, thus reducing the number of search keys that can be held in these nodes. Clearly, *m < n*, but the exact relationship between *m* and *n* depends on the relative size of search key and pointers.

The number of nodes accessed in a lookup in a B-tree depends on where the search key is located. A lookup on a $B^+-tree$ requires traversal of a path from the root of the tree to some leaf node. In contrast, it is sometimes possible to find the desired value in a B-tree before reaching a leaf node. However, roughly *n* times as many keys are stored in the leaf level of a B-tree as in the non-leaf levels, and, since *n* is typically large, the benefit of finding certain values early is relatively small. However, the fact that fewer search keys appear in a non-leaf B-tree node, compared to $B^+-trees$, implies that a B-tree has a smaller fanout and therefore may have depth greater than that of the corresponding $B^+-tree$. Thus, lookup in a B-tree is faster for some search keys but slower for others, although, in general, lookup time is still proportional to the logarithm of the number of search keys.

Deletion in a B-tree is more complicated. In a $B^+-tree$, the deleted entry always appears in a leaf. In a B-tree, the deleted entry may appear in a non-leaf node. The proper value must be selected as a replacement from the subtree of the node containing the deleted entry. Specifically, if search key $K_i$ is deleted, the smallest search key appearing in the subtree of pointer $P_{i + 1}$ must be moved to the field formerly occupied by $K_i$. Further actions need to be taken if the leaf node now has too few entries. In contrast, insertion in a B-tree is only slightly more complicated than is insertion in a $B^+-tree$.

The space advantages of B-trees are marginal for large indices, and usually do not outweigh the disadvantages that we have noted. Thus, pretty much all database-system implementations use the $B^+-tree$ data structure, even if (as we discussed earlier) they refer to the data structure as a B-tree.
## Flash Memory
In our description of indexing so far, we have assumed that data are resident on magnetic disks. Although this assumption continues to be true for the most part, flash memory capacities have grown significantly, making flash memory storage a serious contender for replacing magnetic-disk storage for many applications. A natural question is, how would this change affect the index structure.

Flash-memory storage is structured as blocks, and the $B^+-tree$ index structure can be used for flash-memory storage. The benefit of the much faster access speeds is clear for index lookups. Instead of requiring an average of 10 milliseconds to seek to and read a block, a random block can be read in about a microsecond from flash-memory. Thus lookups run significantly faster than with disk-based data. The optimum $B^+-tree$ node size for flash-memory is typically smaller than that with disk.

The only real drawback with flash memory is that it does not permit in-place updates to data at the physical level, although it appears to do so logically. Every update turns into a copy+write of an entire flash-memory block, requiring the old copy of the block to be erased subsequently; a block erase takes about 1 millisecond. There is ongoing research aimed at developing index structures that can reduce the number of block erases. Meanwhile, standard $B^+-tree$ indices can continue to be used even on flash-memory storage, with acceptable update performance, and significantly improved lookup performance compared to disk storage.
## Related Articles
- [[Chapter 11 - Indexing and Hashing]]

